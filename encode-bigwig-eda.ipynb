{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENCODE bigWig analysis questions\n",
    "\n",
    "Do your best to answer all parts of each question. You are encouraged to collaborate, but should turn in your own answers. \n",
    "\n",
    "Please limit each answer to a maximum of one markdown cell, one code cell and one plot. \n",
    "\n",
    "Put helper functions into a separate script (e.g. `hwutils.py`) so the notebook can be focused on plotting. Also see the [workshop on Clean Code](https://drive.google.com/file/d/1TraVwRkbkCbHq-s_-NS69ZEbRNwH8XNh/view) from Dan Larremore (https://larremorelab.github.io/slides/) for good coding tips to use in this assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I worked with Allen and Yumin (in limited extent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful libraries to import\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import  sklearn.decomposition\n",
    "from sklearn import manifold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import EngFormatter\n",
    "bp_formatter = EngFormatter('b') \n",
    "# nice way to format ticks as human-readable: ax.xaxis.set_major_formatter(bp_formatter)\n",
    "\n",
    "from hwutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataFrame of bigWigs from ENCODE (encodeproject.org/), binned to 10kb resolution across chromosome 10.\n",
    "# note that the first three columns are chrom,start,end and the other columns are labeled by bigWig file accession.\n",
    "df = pd.read_table('./data/ENCODE_GRCh38_binned_subset.tsv')\n",
    "\n",
    "# load metadata from ENCODE for bigwig files. \n",
    "# can be queried as follows: bigwig_metadata.query(\"`File accession`==@ df_column_name \")\n",
    "bigwig_metadata = pd.read_table('./data/ENCODE_GRCh38_bigWig_metadata.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After loading the data (above), and visualize some of the profiles. Why might many signals dip on chr10 at around 40Mb?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "# Iterating through columns and plotting them\n",
    "for col in df.columns[3:]:\n",
    "    plt.plot(df['start'], df[col], label=col)\n",
    "\n",
    "x_position = df.iloc[3999]['start']\n",
    "plt.axvline(x=x_position, color='red', linestyle='--', linewidth=1.5)\n",
    "\n",
    "plt.title('Signal Profiles on Chromosome 10')\n",
    "plt.xlabel('Position on Chromosome 10')\n",
    "plt.ylabel('Signal Value')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A1: I suspect that the region on chromosome 10 with a low ChIP-seq signal corresponds to the centromere. The centromeric region is typically devoid of protein binding relevant for transcriptional activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use scikit-learn to perform PCA, and make a scatterplot of PC1 vs PC2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_to_pca = df.drop(columns=['chrom', 'start', 'end']) \n",
    "data_transposed = data_to_pca.T\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)  # We want the first two principal components\n",
    "principal_components = pca.fit_transform(data_transposed)\n",
    "\n",
    "# Convert the principal components to a DataFrame for easier plotting\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Plotting PC1 vs PC2\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(pca_df['PC1'], pca_df['PC2'], edgecolor='k')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of ENCODE Data: PC1 vs PC2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try to use the experiment metadata to understand and remove outliers. Try labeling or coloring points by various metadata columns. Were any columns in the metadata useful for outlier removal? Note that `sklearn.preprocessing.LabelEncoder()` can be useful for transforming text strings to categories, and `plt.text` can be used to overlay labels with points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "file_accession_to_biosample = dict(zip(bigwig_metadata[\"File accession\"], bigwig_metadata[\"Audit ERROR\"]))\n",
    "\n",
    "biosample_labels = data_transposed.index.map(file_accession_to_biosample)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "color_labels = label_encoder.fit_transform(biosample_labels)\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "sc = plt.scatter(pca_df['PC1'], pca_df['PC2'], c=color_labels, cmap='tab10', edgecolor='k', s=50)  # Changed colormap to 'tab20' for clearer distinction and increased point size\n",
    "\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label('Biosample term name')\n",
    "cbar.set_ticks(range(len(label_encoder.classes_)))\n",
    "cbar.set_ticklabels(label_encoder.classes_)\n",
    "\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()  # Adjust layout for better spacing\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A3. I think PCA with Audit ERROR label is the most useful category. This shows that a major outlier group is in control extremely low read depth, extremely low read depth, and missing control alignments have very different pattern. This makes sense, since they are exceptionally poor quality data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which Assays or Experiment Targets show broad vs narrow patterns? Is this consistent across cell types? Does this relate to the patterns seen in PCA? One way to investigate the characteristic scale is by computing the autocorrelation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_targets = bigwig_metadata[\"Experiment target\"].unique()\n",
    "unique_targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which \"Experiment Targets\" (e.g. histone marks or transcription factors) for which cell types are nearby in this PC1 vs PC2 space? Do any of these proximities have plausible biological interpretations? For example, are any polycomb-related factors in proximity? Illustrate this in a plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a dictionary that maps each file accession to its 'Experiment target'\n",
    "file_accession_to_target = dict(zip(bigwig_metadata[\"File accession\"], bigwig_metadata[\"Experiment target\"]))\n",
    "\n",
    "# 2. Map the data_transposed.index to 'Experiment target'\n",
    "target_labels = data_transposed.index.map(file_accession_to_target)\n",
    "\n",
    "# 3. Color code: \"EZH2\" and \"RBBP5\" in distinct colors, everything else in gray\n",
    "colors = []\n",
    "for label in target_labels:\n",
    "    if label == 'EZH2-human':\n",
    "        colors.append('blue')  # blue for EZH2\n",
    "    elif label == 'RBBP5-human':\n",
    "        colors.append('red')  # red for RBBP5\n",
    "    else:\n",
    "        colors.append('gray')  # gray for others\n",
    "\n",
    "# 4. Plot the PCA data with colors representing the 'Experiment target' of interest\n",
    "plt.figure(figsize=(12, 9))\n",
    "for idx, row in pca_df.iterrows():\n",
    "    plt.scatter(row['PC1'], row['PC2'], color=colors[idx], edgecolor='k', s=50)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label='EZH2', markersize=10, markerfacecolor='blue'),\n",
    "                   Line2D([0], [0], marker='o', color='w', label='RBBP5', markersize=10, markerfacecolor='red'),\n",
    "                   Line2D([0], [0], marker='o', color='w', label='Others', markersize=10, markerfacecolor='gray')]\n",
    "plt.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of ENCODE Data: PC1 vs PC2 colored by Experimental target')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# 1. Map for Experimental targets to shapes\n",
    "target_shape_map = {\n",
    "    'EZH2-human': 'o',\n",
    "    'RBBP5-human': '^',\n",
    "    'Other': 's'\n",
    "}\n",
    "\n",
    "shapes = [target_shape_map.get(target, target_shape_map['Other']) for target in target_labels]\n",
    "\n",
    "# 2. Assign colors for each unique cell type using a colormap\n",
    "colors_map = cm.rainbow(np.linspace(0, 1, len(unique_cell_types)))\n",
    "color_map = dict(zip(unique_cell_types, colors_map))\n",
    "colors = [color_map[cell_type] for cell_type in biosample_labels]\n",
    "\n",
    "# 3. Plot the PCA data\n",
    "plt.figure(figsize=(12, 9))\n",
    "legend_elements = []\n",
    "\n",
    "for idx, row in pca_df.iterrows():\n",
    "    shape = shapes[idx]\n",
    "    color = colors[idx]\n",
    "    if target_labels[idx] in [\"EZH2-human\", \"RBBP5-human\"]:\n",
    "        plt.scatter(row['PC1'], row['PC2'], color=color, edgecolor='k', s=50, marker=shape)\n",
    "    else:\n",
    "        plt.scatter(row['PC1'], row['PC2'], facecolors='none', edgecolor=color, s=50, marker=shape)\n",
    "\n",
    "    # Create legend entries for unique combinations\n",
    "    legend_label = f\"{target_labels[idx]} ({biosample_labels[idx]})\"\n",
    "    if legend_label not in [legend.get_label() for legend in legend_elements]:\n",
    "        if target_labels[idx] in [\"EZH2\", \"RBBP5\"]:\n",
    "            legend_elements.append(Line2D([0], [0], marker=shape, color=color, label=legend_label, markersize=10, markeredgecolor='k', markerfacecolor=color))\n",
    "        else:\n",
    "            legend_elements.append(Line2D([0], [0], marker=shape, color='w', label=legend_label, markersize=10, markeredgecolor=color, markerfacecolor='none'))\n",
    "\n",
    "# Move the legend to a more appropriate location\n",
    "plt.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of ENCODE Data: PC1 vs PC2')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A5. Yes. The polycomb-related factors, including the RBBP5 and EZH-2 are relatively closely located in the PCA clusters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How much does preprocessing matter? Try normalizing the variance per track and see if you arrive at similar or distinct conclusions. Try removing the region on chr10 mentioned above. Note that `sklearn.preprocessing.StandardScaler` could be useful for preprocessing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Preprocess the data: Remove non-numeric columns and standardize\n",
    "data_to_pca = df.drop(columns=['chrom', 'start', 'end']) \n",
    "data_to_pca = data_to_pca.drop(range(3999, 4151))\n",
    "\n",
    "data_standardized = StandardScaler().fit_transform(data_to_pca)\n",
    "data_transposed = data_standardized.T\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)  # We want the first two principal components\n",
    "principal_components = pca.fit_transform(data_transposed)\n",
    "\n",
    "# Convert the principal components to a DataFrame for easier plotting\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Plotting PC1 vs PC2\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(pca_df['PC1'], pca_df['PC2'], edgecolor='k')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of ENCODE Data: PC1 vs PC2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It reduced the issues with the outlier we had previously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How many PCs are needed to explain 90% of the variance in the data? Illustrate this with a scree plot (https://en.wikipedia.org/wiki/Scree_plot). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fit PCA on the data without specifying the number of components\n",
    "pca_full = PCA()\n",
    "pca_full.fit(data_transposed)\n",
    "\n",
    "# Step 2: Calculate the cumulative explained variance\n",
    "explained_variance_ratio = pca_full.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Step 3: Plot the scree plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.5, align='center', label='individual explained variance')\n",
    "plt.step(range(1, len(cumulative_variance) + 1), cumulative_variance, where='mid', label='cumulative explained variance')\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', label=\"90% explained variance\")\n",
    "plt.xlabel('Principal Component Number')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Scree Plot')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Find number of components needed for 90% variance\n",
    "num_components_90_variance = np.where(cumulative_variance >= 0.9)[0][0]\n",
    "print(f\"To explain at least 90% of the variance, {num_components_90_variance} principal components are needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How different is the dimensionality reduction into two dimensions for PCA from that obtained using MDS (multi-dimensional scaling)? What methods could be used to determine the similarity? Illustrate with a plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dissimilarity_matrix = np.sqrt(((data_transposed[:, None] - data_transposed) ** 2).sum(axis=2))\n",
    "mds = manifold.MDS(n_components=2, dissimilarity='precomputed')\n",
    "X_mds = mds.fit_transform(dissimilarity_matrix)\n",
    "\n",
    "# Plot the MDS representation\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_mds[:, 0], X_mds[:, 1],color='red', marker='o')\n",
    "plt.xlabel('MDS Dimension 1')\n",
    "plt.ylabel('MDS Dimension 2')\n",
    "plt.title('MDS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Would non-negative matrix factorization (https://en.wikipedia.org/wiki/Non-negative_matrix_factorization) be a useful method to use for this dataset? Why or why not?  (No plots needed for this question).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF works best with datasets that only have positive values. It breaks down data into simpler parts that are easier to understand. For the ENCODE dataset, if all the data points are positive and we're looking for simpler, combined representations, NMF could work. It can be more advantageous because it can show additive patterns in the data that PCA cannot. I don't think the data itself contains negative values, so it should be possible to use implement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
