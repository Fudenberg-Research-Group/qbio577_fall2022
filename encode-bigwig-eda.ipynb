{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENCODE bigWig analysis questions\n",
    "\n",
    "Do your best to answer all parts of each question. You are encouraged to collaborate, but should turn in your own answers. \n",
    "\n",
    "Please limit each answer to a maximum of one markdown cell, one code cell and one plot. \n",
    "\n",
    "Put helper functions into a separate script (e.g. `hwutils.py`) so the notebook can be focused on plotting. Also see the [workshop on Clean Code](https://drive.google.com/file/d/1TraVwRkbkCbHq-s_-NS69ZEbRNwH8XNh/view) from Dan Larremore (https://larremorelab.github.io/slides/) for good coding tips to use in this assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I collaborated with Allen, Yumin, and Qifan (in limited extent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful libraries to import\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import  sklearn.decomposition\n",
    "from sklearn import manifold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import EngFormatter\n",
    "bp_formatter = EngFormatter('b') \n",
    "# nice way to format ticks as human-readable: ax.xaxis.set_major_formatter(bp_formatter)\n",
    "\n",
    "from hwutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataFrame of bigWigs from ENCODE (encodeproject.org/), binned to 10kb resolution across chromosome 10.\n",
    "# note that the first three columns are chrom,start,end and the other columns are labeled by bigWig file accession.\n",
    "df = pd.read_table('./data/ENCODE_GRCh38_binned_subset.tsv')\n",
    "\n",
    "# load metadata from ENCODE for bigwig files. \n",
    "# can be queried as follows: bigwig_metadata.query(\"`File accession`==@ df_column_name \")\n",
    "bigwig_metadata = pd.read_table('./data/ENCODE_GRCh38_bigWig_metadata.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After loading the data (above), and visualize some of the profiles. Why might many signals dip on chr10 at around 40Mb?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "# Iterating through columns and plotting them\n",
    "for col in df.columns[3:]:\n",
    "    plt.plot(df['start'], df[col], label=col)\n",
    "\n",
    "x_position = df.iloc[3999]['start']\n",
    "plt.axvline(x=x_position, color='red', linestyle='--', linewidth=1.5)\n",
    "\n",
    "plt.title('Signal Profiles on Chromosome 10')\n",
    "plt.xlabel('Position on Chromosome 10')\n",
    "plt.ylabel('Signal Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A1: I suspect that the region on chromosome 10 with a low ChIP-seq signal corresponds to the centromere. The centromeric region is typically devoid of protein binding relevant for transcriptional activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use scikit-learn to perform PCA, and make a scatterplot of PC1 vs PC2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_to_pca = df.drop(columns=['chrom', 'start', 'end']) \n",
    "data_transposed = data_to_pca.T\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)  # We want the first two principal components\n",
    "principal_components = pca.fit_transform(data_transposed)\n",
    "\n",
    "# Convert the principal components to a DataFrame for easier plotting\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Plotting PC1 vs PC2\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(pca_df['PC1'], pca_df['PC2'], edgecolor='k')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of ENCODE Data: PC1 vs PC2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try to use the experiment metadata to understand and remove outliers. Try labeling or coloring points by various metadata columns. Were any columns in the metadata useful for outlier removal? Note that `sklearn.preprocessing.LabelEncoder()` can be useful for transforming text strings to categories, and `plt.text` can be used to overlay labels with points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "file_accession_to_biosample = dict(zip(bigwig_metadata[\"File accession\"], bigwig_metadata[\"Audit ERROR\"]))\n",
    "\n",
    "biosample_labels = data_transposed.index.map(file_accession_to_biosample)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "color_labels = label_encoder.fit_transform(biosample_labels)\n",
    "\n",
    "# Get unique biosample labels and assign a color to each\n",
    "unique_biosamples = label_encoder.classes_\n",
    "colors = plt.cm.get_cmap('tab10', len(unique_biosamples))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "for biosample, color_idx in zip(unique_biosamples, range(len(unique_biosamples))):\n",
    "    mask = color_labels == color_idx\n",
    "    ax.scatter(pca_df['PC1'][mask], pca_df['PC2'][mask], color=colors(color_idx), edgecolor='k', s=50, label=biosample)\n",
    "\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.grid(True)\n",
    "\n",
    "# Create a legend and move it outside\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', markerfacecolor=colors(color_idx), markersize=10, label=biosample) for biosample, color_idx in zip(unique_biosamples, range(len(unique_biosamples)))]\n",
    "ax.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A3. I think PCA with Audit ERROR label is the most useful category. This shows that a major outlier group is in control extremely low read depth, extremely low read depth, and missing control alignments have very different pattern. This makes sense, since they are exceptionally poor quality data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which Assays or Experiment Targets show broad vs narrow patterns? Is this consistent across cell types? Does this relate to the patterns seen in PCA? One way to investigate the characteristic scale is by computing the autocorrelation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Define a function to compute autocorrelation\n",
    "def autocorr(x):\n",
    "    result = np.correlate(x, x, mode='full')\n",
    "    normed_result = result[result.size // 2:] / result[result.size // 2]\n",
    "    return normed_result\n",
    "\n",
    "# Define a function to compute the half-life of autocorrelation\n",
    "def compute_half_life(ac_values):\n",
    "    initial_value = ac_values[0]\n",
    "    half_value = initial_value / 2\n",
    "    for lag, value in enumerate(ac_values):\n",
    "        if value <= half_value:\n",
    "            return lag\n",
    "    return None\n",
    "\n",
    "# Filter the bigwig_metadata DataFrame based on df columns\n",
    "filtered_bigwig_metadata = bigwig_metadata[bigwig_metadata['File accession'].isin(df.columns)]\n",
    "\n",
    "# Create a dictionary where keys are experiment targets and values are lists of associated file accessions\n",
    "file_target_to_accession = {}\n",
    "for _, row in filtered_bigwig_metadata.iterrows():\n",
    "    target = row['Experiment target']\n",
    "    accession = row['File accession']\n",
    "    if target not in file_target_to_accession:\n",
    "        file_target_to_accession[target] = []\n",
    "    file_target_to_accession[target].append(accession)\n",
    "\n",
    "# Compute autocorrelation for each target\n",
    "autocorr_values = {}\n",
    "for target, accessions in file_target_to_accession.items():\n",
    "    combined_series = np.zeros(df.shape[0])\n",
    "    for accession in accessions:\n",
    "        combined_series += df[accession].values\n",
    "    ac = autocorr(combined_series / len(accessions))\n",
    "    autocorr_values[target] = ac\n",
    "\n",
    "# Compute half-life for each target\n",
    "half_life_values = {}\n",
    "for target, values in autocorr_values.items():\n",
    "    half_life = compute_half_life(values)\n",
    "    half_life_values[target] = half_life\n",
    "\n",
    "# Define a threshold for categorizing as broad or narrow\n",
    "half_life_threshold = np.median(list(half_life_values.values()))\n",
    "\n",
    "# Categorize targets as broad or narrow based on the threshold\n",
    "pattern_classification_half_life = {}\n",
    "for target, half_life in half_life_values.items():\n",
    "    if half_life <= half_life_threshold:\n",
    "        pattern_classification_half_life[target] = \"narrow\"\n",
    "    else:\n",
    "        pattern_classification_half_life[target] = \"broad\"\n",
    "\n",
    "# Print the pattern classification\n",
    "print(pattern_classification_half_life)\n",
    "\n",
    "# Map file accessions to their cell type\n",
    "file_accession_to_celltype = dict(zip(bigwig_metadata[\"File accession\"], bigwig_metadata[\"Biosample term name\"]))\n",
    "\n",
    "# Map data_transposed index to target labels\n",
    "celltype_labels = data_transposed.index.map(file_accession_to_celltype)\n",
    "\n",
    "# Encode the cell type labels for color-coding\n",
    "label_encoder = LabelEncoder()\n",
    "color_labels = label_encoder.fit_transform(celltype_labels)\n",
    "\n",
    "# Map pattern labels ('broad' or 'narrow')\n",
    "pattern_labels = [pattern_classification[file_accession_to_target[accession]] for accession in data_transposed.index]\n",
    "\n",
    "# Define markers for 'broad' and 'narrow'\n",
    "markers = ['o' if pattern == \"broad\" else 'x' for pattern in pattern_labels]\n",
    "\n",
    "# Use a distinct color palette (tab20b)\n",
    "colors = plt.cm.get_cmap(\"tab20b\", len(label_encoder.classes_))\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "# Scatter plot points with different colors and markers based on cell type and pattern\n",
    "for color, celltype in zip(colors(range(len(label_encoder.classes_))), label_encoder.classes_):\n",
    "    for marker in ['o', 'x']:\n",
    "        mask = np.logical_and(np.array(markers) == marker, np.array(celltype_labels) == celltype)\n",
    "        marker_size = 100 if marker == 'o' else 50\n",
    "        # Add alpha (transparency) to the markers\n",
    "        ax.scatter(pca_df['PC1'][mask], pca_df['PC2'][mask], color=color, edgecolor=color, s=marker_size, marker=marker, alpha=0.5)\n",
    "\n",
    "# Set labels for the x and y axes\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "\n",
    "# Create a legend for pattern (Broad and Narrow)\n",
    "pattern_legend = [Line2D([0], [0], color='gray', marker='o', linestyle='None', markersize=10),\n",
    "                  Line2D([0], [0], color='gray', marker='x', linestyle='None', markersize=6)]\n",
    "legend1 = ax.legend(pattern_legend, ['Broad', 'Narrow'], loc=\"upper right\")\n",
    "\n",
    "# Create a legend for cell types and move it outside the plot\n",
    "legend2_elements = [Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10, label=celltype) for color, celltype in zip(colors(range(len(label_encoder.classes_))), label_encoder.classes_)]\n",
    "legend2 = ax.legend(handles=legend2_elements, loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Add both legends back to the plot\n",
    "ax.add_artist(legend1)\n",
    "ax.add_artist(legend2)\n",
    "\n",
    "# Add grid lines\n",
    "ax.grid(True)\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A4: In the provided code, autocorrelation analysis was used to investigate whether Experiment Targets exhibited \"broad\" or \"narrow\" patterns based on the decay rate (higher vs. lower than the median). This analysis categorized Experiment Targets based on the half-life of their autocorrelation functions, which quantifies the characteristic scale of temporal correlation. The results were then visualized in a scatter plot, with cell types color-coded and patterns denoted by markers. \n",
    "\n",
    "Clearly, the experimental targets with broader patterns cluster together, roughly irrespective of the specific cell types they come from (although you can also see cell type-specific clustering as well). On the other hand, the narrow pattern targets scatter widely, often clustering in different parts of the PCA space. Here, cell-type specific clustering pattern is also observed, although broad vs. narrow generally segregate regardless of the specific cell type information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which \"Experiment Targets\" (e.g. histone marks or transcription factors) for which cell types are nearby in this PC1 vs PC2 space? Do any of these proximities have plausible biological interpretations? For example, are any polycomb-related factors in proximity? Illustrate this in a plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a dictionary that maps each file accession to its 'Experiment target'\n",
    "file_accession_to_target = dict(zip(bigwig_metadata[\"File accession\"], bigwig_metadata[\"Experiment target\"]))\n",
    "\n",
    "# 2. Map the data_transposed.index to 'Experiment target'\n",
    "target_labels = data_transposed.index.map(file_accession_to_target)\n",
    "\n",
    "# 3. Color code: \"EZH2\" and \"RBBP5\" in distinct colors, everything else in gray\n",
    "colors = []\n",
    "marker_sizes = []  # Initialize a list for marker sizes\n",
    "for label in target_labels:\n",
    "    if label == 'EZH2-human':\n",
    "        colors.append('blue')  # blue for EZH2\n",
    "        marker_sizes.append(100)  # Increase marker size for EZH2\n",
    "    elif label == 'RBBP5-human':\n",
    "        colors.append('red')  # red for RBBP5\n",
    "        marker_sizes.append(100)  # Increase marker size for RBBP5\n",
    "    else:\n",
    "        colors.append('gray')  # gray for others\n",
    "        marker_sizes.append(30)  # Reduce marker size for others\n",
    "\n",
    "# 4. Plot the PCA data with colors and marker sizes representing the 'Experiment target' of interest\n",
    "plt.figure(figsize=(12, 9))\n",
    "for idx, row in pca_df.iterrows():\n",
    "    plt.scatter(row['PC1'], row['PC2'], color=colors[idx], edgecolor='k', s=marker_sizes[idx])\n",
    "\n",
    "# Legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label='EZH2', markersize=10, markerfacecolor='blue'),\n",
    "                   Line2D([0], [0], marker='o', color='w', label='RBBP5', markersize=10, markerfacecolor='red'),\n",
    "                   Line2D([0], [0], marker='o', color='w', label='Others', markersize=10, markerfacecolor='gray')]\n",
    "plt.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of ENCODE Data: PC1 vs PC2 colored by Experimental target')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A5. Yes. The polycomb-related factors, including the RBBP5 and EZH-2 are relatively closely located in the PCA clusters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How much does preprocessing matter? Try normalizing the variance per track and see if you arrive at similar or distinct conclusions. Try removing the region on chr10 mentioned above. Note that `sklearn.preprocessing.StandardScaler` could be useful for preprocessing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Preprocess the data: Remove non-numeric columns and standardize\n",
    "data_to_pca = df.drop(columns=['chrom', 'start', 'end']) \n",
    "data_to_pca = data_to_pca.drop(range(3999, 4151))\n",
    "\n",
    "data_standardized = StandardScaler().fit_transform(data_to_pca)\n",
    "data_transposed = data_standardized.T\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)  # We want the first two principal components\n",
    "principal_components = pca.fit_transform(data_transposed)\n",
    "\n",
    "# Convert the principal components to a DataFrame for easier plotting\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Plotting PC1 vs PC2\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(pca_df['PC1'], pca_df['PC2'], edgecolor='k')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of ENCODE Data: PC1 vs PC2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A6: It reduced the issues with the outlier we had previously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How many PCs are needed to explain 90% of the variance in the data? Illustrate this with a scree plot (https://en.wikipedia.org/wiki/Scree_plot). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fit PCA on the data without specifying the number of components\n",
    "pca_full = PCA()\n",
    "pca_full.fit(data_transposed)\n",
    "\n",
    "# Step 2: Calculate the cumulative explained variance\n",
    "explained_variance_ratio = pca_full.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Step 3: Plot the scree plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.5, align='center', label='individual explained variance')\n",
    "plt.step(range(1, len(cumulative_variance) + 1), cumulative_variance, where='mid', label='cumulative explained variance')\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', label=\"90% explained variance\")\n",
    "plt.xlabel('Principal Component Number')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Scree Plot')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Find number of components needed for 90% variance\n",
    "num_components_90_variance = np.where(cumulative_variance >= 0.9)[0][0]\n",
    "print(f\"A7. To explain at least 90% of the variance, {num_components_90_variance} principal components are needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How different is the dimensionality reduction into two dimensions for PCA from that obtained using MDS (multi-dimensional scaling)? What methods could be used to determine the similarity? Illustrate with a plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the dissimilarity matrix based on Euclidean distances between data points\n",
    "dissimilarity_matrix = np.sqrt(((data_transposed[:, None] - data_transposed) ** 2).sum(axis=2))\n",
    "\n",
    "# Apply Multi-Dimensional Scaling (MDS) to reduce the dimensionality to two dimensions\n",
    "mds = manifold.MDS(n_components=2, dissimilarity='precomputed')\n",
    "X_mds = mds.fit_transform(dissimilarity_matrix)\n",
    "\n",
    "# Plot the MDS representation\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_mds[:, 0], X_mds[:, 1], color='red', marker='o')\n",
    "plt.xlabel('MDS Dimension 1')\n",
    "plt.ylabel('MDS Dimension 2')\n",
    "plt.title('MDS')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Would non-negative matrix factorization (https://en.wikipedia.org/wiki/Non-negative_matrix_factorization) be a useful method to use for this dataset? Why or why not?  (No plots needed for this question).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A9: NMF works best with datasets that only have positive values. It breaks down data into simpler parts that are easier to understand. For the ENCODE dataset, if all the data points are positive and we're looking for simpler, combined representations, NMF could work. It can be more advantageous because it can show additive patterns in the data that PCA cannot. I don't think the data itself contains negative values, so it should be possible to use implement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
