{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENCODE bigWig analysis questions\n",
    "\n",
    "Do your best to answer all parts of each question. You are encouraged to collaborate, but should turn in your own answers. \n",
    "\n",
    "Please limit each answer to a maximum of one markdown cell, one code cell and one plot. \n",
    "\n",
    "Put helper functions into a separate script (e.g. `hwutils.py`) so the notebook can be focused on plotting. Also see the [workshop on Clean Code](https://drive.google.com/file/d/1TraVwRkbkCbHq-s_-NS69ZEbRNwH8XNh/view) from Dan Larremore (https://larremorelab.github.io/slides/) for good coding tips to use in this assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I worked with Allen and Yumin (in limited extent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful libraries to import\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import  sklearn.decomposition\n",
    "from sklearn import manifold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import EngFormatter\n",
    "bp_formatter = EngFormatter('b') \n",
    "# nice way to format ticks as human-readable: ax.xaxis.set_major_formatter(bp_formatter)\n",
    "\n",
    "from hwutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataFrame of bigWigs from ENCODE (encodeproject.org/), binned to 10kb resolution across chromosome 10.\n",
    "# note that the first three columns are chrom,start,end and the other columns are labeled by bigWig file accession.\n",
    "df = pd.read_table('./data/ENCODE_GRCh38_binned_subset.tsv')\n",
    "\n",
    "# load metadata from ENCODE for bigwig files. \n",
    "# can be queried as follows: bigwig_metadata.query(\"`File accession`==@ df_column_name \")\n",
    "bigwig_metadata = pd.read_table('./data/ENCODE_GRCh38_bigWig_metadata.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After loading the data (above), and visualize some of the profiles. Why might many signals dip on chr10 at around 40Mb?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "# Iterating through columns and plotting them\n",
    "for col in df.columns[3:]:\n",
    "    plt.plot(df['start'], df[col], label=col)\n",
    "\n",
    "x_position = df.iloc[3999]['start']\n",
    "plt.axvline(x=x_position, color='red', linestyle='--', linewidth=1.5)\n",
    "\n",
    "plt.title('Signal Profiles on Chromosome 10')\n",
    "plt.xlabel('Position on Chromosome 10')\n",
    "plt.ylabel('Signal Value')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A1: I suspect that the region on chromosome 10 with a low ChIP-seq signal corresponds to the centromere. The centromeric region is typically devoid of protein binding relevant for transcriptional activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use scikit-learn to perform PCA, and make a scatterplot of PC1 vs PC2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_to_pca = df.drop(columns=['chrom', 'start', 'end']) \n",
    "data_transposed = data_to_pca.T\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)  # We want the first two principal components\n",
    "principal_components = pca.fit_transform(data_transposed)\n",
    "\n",
    "# Convert the principal components to a DataFrame for easier plotting\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Plotting PC1 vs PC2\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(pca_df['PC1'], pca_df['PC2'], edgecolor='k')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of ENCODE Data: PC1 vs PC2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try to use the experiment metadata to understand and remove outliers. Try labeling or coloring points by various metadata columns. Were any columns in the metadata useful for outlier removal? Note that `sklearn.preprocessing.LabelEncoder()` can be useful for transforming text strings to categories, and `plt.text` can be used to overlay labels with points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bigwig_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A3. I think PCA with Biosample term name label is the most useful category. This represents the sample's cell type, and adheres well to the outlier patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which Assays or Experiment Targets show broad vs narrow patterns? Is this consistent across cell types? Does this relate to the patterns seen in PCA? One way to investigate the characteristic scale is by computing the autocorrelation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A4. I'm not sure what it exactly mean by broad vs narrow patterns, but TF-ChIP-seq seem to show narrower (i.e., more clustered) pattern than the Histone-ChIP-seq. I think it is more or less consistent throughout cell type.\n",
    "As for the experimental target, there seems to be a general clustering regardless of specific cell type the sample is coming from, except few examples such as H3K4me3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which \"Experiment Targets\" (e.g. histone marks or transcription factors) for which cell types are nearby in this PC1 vs PC2 space? Do any of these proximities have plausible biological interpretations? For example, are any polycomb-related factors in proximity? Illustrate this in a plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A5. Yes. The polycomb-related factors, including the RBBP5 and EZH-2 are relatively closely located in the PCA clusters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How much does preprocessing matter? Try normalizing the variance per track and see if you arrive at similar or distinct conclusions. Try removing the region on chr10 mentioned above. Note that `sklearn.preprocessing.StandardScaler` could be useful for preprocessing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Preprocess the data: Remove non-numeric columns and standardize\n",
    "data_to_pca = df.drop(columns=['chrom', 'start', 'end']) \n",
    "data_to_pca = data_to_pca.drop(range(3999, 4151))\n",
    "\n",
    "data_standardized = StandardScaler().fit_transform(data_to_pca)\n",
    "data_transposed = data_standardized.T\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)  # We want the first two principal components\n",
    "principal_components = pca.fit_transform(data_transposed)\n",
    "\n",
    "# Convert the principal components to a DataFrame for easier plotting\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Plotting PC1 vs PC2\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(pca_df['PC1'], pca_df['PC2'], edgecolor='k')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of ENCODE Data: PC1 vs PC2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It reduced the issues with the outlier we had previously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How many PCs are needed to explain 90% of the variance in the data? Illustrate this with a scree plot (https://en.wikipedia.org/wiki/Scree_plot). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fit PCA on the data without specifying the number of components\n",
    "pca_full = PCA()\n",
    "pca_full.fit(data_transposed)\n",
    "\n",
    "# Step 2: Calculate the cumulative explained variance\n",
    "explained_variance_ratio = pca_full.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Step 3: Plot the scree plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.5, align='center', label='individual explained variance')\n",
    "plt.step(range(1, len(cumulative_variance) + 1), cumulative_variance, where='mid', label='cumulative explained variance')\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', label=\"90% explained variance\")\n",
    "plt.xlabel('Principal Component Number')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Scree Plot')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Find number of components needed for 90% variance\n",
    "num_components_90_variance = np.where(cumulative_variance >= 0.9)[0][0]\n",
    "print(f\"To explain at least 90% of the variance, {num_components_90_variance} principal components are needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How different is the dimensionality reduction into two dimensions for PCA from that obtained using MDS (multi-dimensional scaling)? What methods could be used to determine the similarity? Illustrate with a plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dissimilarity_matrix = np.sqrt(((data_transposed[:, None] - data_transposed) ** 2).sum(axis=2))\n",
    "mds = manifold.MDS(n_components=2, dissimilarity='precomputed')\n",
    "X_mds = mds.fit_transform(dissimilarity_matrix)\n",
    "\n",
    "# Plot the MDS representation\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_mds[:, 0], X_mds[:, 1],color='red', marker='o')\n",
    "plt.xlabel('MDS Dimension 1')\n",
    "plt.ylabel('MDS Dimension 2')\n",
    "plt.title('MDS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Would non-negative matrix factorization (https://en.wikipedia.org/wiki/Non-negative_matrix_factorization) be a useful method to use for this dataset? Why or why not?  (No plots needed for this question).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF works best with datasets that only have positive values. It breaks down data into simpler parts that are easier to understand. For the ENCODE dataset, if all the data points are positive and we're looking for simpler, combined representations, NMF could work. It can be more advantageous because it can show additive patterns in the data that PCA cannot. I don't think the data itself contains negative values, so it should be possible to use implement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
